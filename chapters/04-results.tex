\chapter{Results}\label{chp:results}

In this chapter, we present the results of different networks. Sections from \ref{sec:res} to \ref{sec:gogl} present the results for each network. Every section is divided in two subsection: one in which we discuss the results of different initialization and one in which the results for different classes are presented. In Section \ref{sec:opt}, we present the results for a different optimizer and three learning rates. 
\section{ResNet-18}\label{sec:res}
\subsection{Initialization}
For every class, we tested our network with either random initialization or with using pretrained weights. In Table \ref{tab:resinit}, we can see the best accuracy on the validation set and the number of epochs for training for each class.  

\begin{table}
\caption[Results according to initialization of ResNet-18]{\label{tab:resinit} The results of training ResNet-18 using random initialization or pretrained weights with Adamax as optimizer. The score is the validation accuracy for Dunham, Porosity and \gls{drt}. It is the F1-score for the Components class.}
\centering
\begin{tabular}[b]{| l | l | l | l | l |}
\hline
    Initialization & Class & Validation score & Epochs\ \\ \hline
    \multirow{4}{*}{Pretrained Weights} & Dunham &  60,7\%  & 50 \\ 
    & Porosity & 73,8\%  &  60 \\
    &DRT & 52,1\% &  50 \\
    &Components & 69,1\% &  50 \\ \hline
     \multirow{4}{*}{Random} & Dunham &  44,8\%  & 50 \\
    & Porosity & 63,4\% &  50 \\
    &DRT & 27,6\% & 50 \\
    &Components & 34,1\% &  50 \\ \hline
    
\end{tabular} 
\end{table}
In Figure \ref{fig:plotres}, we plotted the accuracy, F1 score, and average training and validation loss for each class. 


\begin{figure}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-Init_poro_acc.PNG}
\caption{ResNet 18 trained on the Porosity class.}
\label{fig:resinitporo}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-Init_dunham_acc.PNG}
\caption{ResNet-18 trained on the Dunham class.}
\label{fig:resinit_dunham}
\end{subfigure}%
}\\
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-Init_drt_acc.PNG}
\caption{ResNet-18 trained on the DRT class.}
\label{fig:resinit_drt}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-Init_components_acc.PNG}
\caption{ResNet-18 trained on the Components class.}
\label{fig:resinit_comp}
\end{subfigure}%
}
\caption[Training and validation plots for ResNet-18]{The lines in green and red are for pretrained weights and yellow and blue for random initialization. The bottom plot is the validation accuracy for plots (a), (b) and (c) and the F1-score for plot (d). The top plot is training  and validation loss. At the top of the top plots, we show the value toward which the score converges.}
\label{fig:plotres}
\end{figure}

\subsection{Classification}
In Table \ref{tab:resbest}, we summarize the best validation and test accuracy or F-1 score for every class. Then we present the confusion matrix for the best performing model and the test data set for the single label classification in Figure \ref{fig:rescm}. For the Components class, we show the statistics for every class in Figure \ref{fig:perf_res}. 

\begin{table}
\caption[Scores of best performing ResNet-18]{\label{tab:resbest} The results of the best version of the ResNet-18 on the classification task. The validation and test accuracy are used as score on the single label classification while F1-scores are used for multi-label classification.}
\centering
\begin{tabular}[b]{| l | l | l | l | l |}
\hline
    Initialization & Class & Validation score & Test score \ \\ \hline
    Pretrained Weights & Dunham &  60,7\%  & 37,3\% \\ \hline
    Pretrained Weights & Porosity & 73,8\%  &  58,5\% \\ \hline
    Pretrained Weights &DRT & 52,1\% &  22,1\% \\ \hline
    Pretrained Weights &Components & 69,1\% &  44,7\% \\ \hline
\end{tabular} 
\end{table}

\begin{figure}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-baby_best.PNG}
\caption{Best for the Porosity class.}
\label{fig:rescm_poro}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-baby_pred.PNG}
\caption{Predicted for the Porosity class.}
\label{fig:rescm_poro_pred}
\end{subfigure}%
}\\
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-dunham_best.PNG}
\caption{Best for the Dunham class.}
\label{fig:rescm_dunham}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-dunham_pred.PNG}
\caption{Predicted for the Dunham class.}
\label{fig:rescm_dunham_pred}
\end{subfigure}%
}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-drt_best.PNG}
\caption{Best for the DRT class.}
\label{fig:rescm_drt}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-drt_pred.PNG}
\caption{Predicted for the DRT class.}
\label{fig:rescm_drt_pred}
\end{subfigure}%
}
\caption[Confusion matrices of classes trained on ResNet-18]{On the left is the confusion matrix of the best performing ResNet-18 and on the right is the confusion matrix on the test set.}
\label{fig:rescm}
\end{figure}

\begin{figure}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-compo_best.PNG}
\caption{Best for the Components class.}
\label{fig:rescm_comp}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-compo_pred.PNG}
\caption{Predicted for the Components class.}
\label{fig:rescm_comp_pred}
\end{subfigure}%
}
\caption [Performance summary of the Components class trained on ResNet-18]{These figure are the summaries of the performance for the multi-label classification. It shows precision, recall, F1-score and also the number of samples for each class.}
\label{fig:perf_res}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ALEX   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{AlexNet}\label{sec:aleX}
\subsection{Initialization}
For every class, we tested our network with either random initialization or with using pretrained weights. In Table \ref{tab:alexinit}, we can see the best accuracy on validation and test set, the number of epochs for training for each class.  
In Figure \ref{fig:plotsalex}, we plotted the accuracy, F1 score, and average training and validation loss for each class. 
\begin{table}
\caption[Results according to initialization of AlexNet]{\label{tab:alexinit} The results of training AlexNet using random initialization or pretrained weights with Adamax as optimizer}
\centering
\begin{tabular}[b]{| l | l | l | l | l |}
\hline
    Initialization & Class & Validation accuracy  & Epochs\\ \hline
    \multirow{4}{*}{Pretrained Weights} & Dunham &  81,7\%  & 50 \\ %%cf Confusion Matrix 
    & Porosity & 88,5\% &  50 \\
    &\gls{drt} & 74,5\% &  50 \\
    &Components & 85,1\% &  50 \\ \hline
     \multirow{4}{*}{Random} & Dunham &  62,1\% & 50 \\
    & Porosity & 64,8\% &  50 \\
    &\gls{drt} & 60,7\% &  50 \\
    &Components & 72,3\% & 50 \\ \hline
\end{tabular} 
\end{table}

\begin{figure}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-Init_al_poro_acc.PNG}

\caption{AlexNet trained on the Porosity class.}
\label{fig:alexinit_poro}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-Init_al_dunham_acc.PNG}
\caption{AlexNet trained on the Dunham class.}
\label{fig:alexinit_dunham}
\end{subfigure}%
}\\
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-Init_al_drt_acc.PNG}
\caption{AlexNet trained on the \gls{drt} class.}
\label{fig:alexinit_drt}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-al_components_acc.PNG}
\caption{AlexNet trained on the Components class.}
\label{fig:alexinit_comp}
\end{subfigure}%
}
\caption[Training and validation plots for AlexNet]{The lines in green and red are for pretrained weights and yellow and blue for random initialization. The bottom plot is the validation accuracy for plots (a), (b) and (c) and the F1-score for plot (d). The top plot is training  and validation loss. At the top of the top plots, we show the value toward which the score converges.}
\label{fig:plotsalex}
\end{figure}


\subsection{Classification}
In Table \ref{tab:alexbest}, we summarize the best validation and test accuracy or F-1 score for every class. Then we present the confusion matrix for the best performing model and the test data set for the single label classification in Figure \ref{fig:alcm}. For the Components class, we show the statistics for every class in Figure \ref{fig:perf_al}. 

\begin{table}
\caption[Scores of best performing AlexNet]{\label{tab:alexbest} The results of the best version of the AlexNet on the classification task. The validation and test accuracy are used as score on the single label classification while F1-scores are used for multi-label classification.}
\centering
\begin{tabular}[b]{| l | l | l | l | l |}
\hline
    Initialization & Class & Validation score & Test score  \\ \hline
    Pretrained Weights & Dunham &  81,7\%  & 36,8\% \\ \hline
    Pretrained Weights & Porosity & 88,5\%  &  69,5\% \\ \hline
    Pretrained Weights &\gls{drt} & 74,5\% &  23,4\% \\ \hline
    Pretrained Weights &Components & 85,1\% &  50,2\% \\ \hline
\end{tabular} 
\end{table}

\begin{figure}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-al_baby_best.PNG}
\caption{Best for the Porosity class.}
\label{fig:alcm_poro}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-al_baby_pred.PNG}
\caption{Predicted for the Porosity class.}
\label{fig:alcm_poro_pred}
\end{subfigure}%
}\\
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-al_dunham_best.PNG}
\caption{Best for the Dunham class.}
\label{fig:alcm_dunham}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-al_dunham_pred.PNG}
\caption{Predicted for the Dunham class.}
\label{fig:alcm_dunham_pred}
\end{subfigure}%
}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-al_drt_best.PNG}
\caption{Best for the \gls{drt} class.}
\label{fig:alcm_drt}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-al_drt_pred.PNG}
\caption{Predicted for the \gls{drt} class.}
\label{fig:alcm_drt_pred}
\end{subfigure}%
}
\caption[Confusion matrices of classes trained on AlexNet]{On the left is the confusion matrix of the best performing AlexNet and on the right is the confusion matrix on the test set.}
\label{fig:alcm}
\end{figure}
\begin{figure}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-al_compo_best.PNG}
\caption{Best for the Components class.}
\label{fig:alcm_comp}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-al_compo_pred.PNG}
\caption{Predicted for the Components class.}
\label{fig:alcm_comp_pred}
\end{subfigure}%
}
\caption [Performance summary of the Components class trained on AlexNet]{These figure are the summaries of the performance for the multi-label classification trained on AlexNet. It shows precision, recall, F1-score and also the number of samples for each class.}
\label{fig:perf_al}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   GOOGLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Inception\_v3}\label{sec:gogl}
\subsection{Initialization}
For every class, we tested our network with either random initialization or with using pretrained weights. In Table \ref{tab:googinit}, we can see the best accuracy on validation and test set, the number of epochs for training for each class.  
In Figure \ref{fig:plotsgoogl}, we plotted the accuracy, F1 score, and average training and validation loss for each class. 

\begin{table}
\caption[Results according to initialization of Inception\_v3]{\label{tab:googinit} The results of training Inception\_v3 network using random initialization or pretrained weights with Adamax as optimizer}
\centering
\begin{tabular}[b]{| l | l | l | l | l |}
\hline
    Initialization & Class & Validation score  & Epochs\\ \hline
    \multirow{4}{*}{Pretrained} & Dunham &  61,1\%  & 50 \\ %%cf Confusion Matrix 
    & Porosity & 76,1\% &  50 \\
    &\gls{drt} & 57,9\% &  50 \\
    &Components & 70,2\% &  50 \\ \hline
     \multirow{4}{*}{Random} & Dunham &  90,4\% & 50 \\
    & Porosity & 97,5\% &  50 \\
    &\gls{drt} & 99,2\% &  50 \\
    &Components & 96,3\% & 50 \\ \hline
\end{tabular} 
\end{table}

\begin{figure}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_poro_acc.PNG}
\caption{Inception\_v3 trained on the Porosity class.}
\label{fig:googinit_poro}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_dunham_acc.PNG}
\caption{Inception\_v3 trained on the Dunham class.}
\label{fig:googinit_dunham}
\end{subfigure}%
}\\
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_drt_acc.PNG}
\caption{Inception\_v3 trained on the \gls{drt} class.}
\label{fig:googinit_drt}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_conponents_acc.PNG}

\caption{Inception\_v3 trained on the Components class.}
\label{fig:googinit_comp}
\end{subfigure}%
}
\caption[Training and validation plots for Inception\_v3]{The lines in green and red are for pretrained weights and yellow and blue for random initialization. The bottom plot is the validation accuracy for plots (a), (b) and (c) and the F1-score for plot (d). The top plot is training  and validation loss. At the top of the top plots, we show the value toward which the score converges.}
\label{fig:plotsgoogl}
\end{figure}


\subsection{Classification}
In Table \ref{tab:googbest}, we summarize the best validation and test accuracy or F-1 score for every class. Then we present the confusion matrix for the best performing model and the test data set for the single label classification in Figure \ref{fig:gocm}. For the Components class, we show the statistics for every class in Figure \ref{fig:perf_goog}. 

\begin{table}
\caption[Scores of best performing Inception\_v3]{\label{tab:googbest} The results of the best version of the Inception\_v3 on the classification task. The validation and test accuracy are used as score on the single label classification while F1-scores are used for multi-label classification.}
\centering
\begin{tabular}[b]{| l | l | l | l | l |}
\hline
    Initialization & Class & Validation score & Test score \ \\ \hline
    Random & Dunham &  90,4\%  & 66,2\% \\ \hline
    Random & Porosity &  97,5\%  & 76,1\% \\ \hline
    Random &\gls{drt} & 99,2\% & 53,7\% \\ \hline
    Random &Components & 96,3\% &  44,6\% \\ \hline
\end{tabular} 
\end{table}

\begin{figure}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_baby_best.PNG}
\label{fig:gocm_poro}
\caption{Best for the Porosity class.}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_baby_pred.PNG}
\caption{Predicted for the Porosity class.}
\label{fig:gocm_poro_pred}
\end{subfigure}%
}\\
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_dunham_best.PNG}
\caption{Best for the Dunham class.}
\label{fig:gocm_dunham}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_dunham_pred.PNG}
\caption{Predicted for the Dunham class.}
\label{fig:gocm_dunham_pred}
\end{subfigure}%
}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_drt_best.PNG}
\caption{Best for the \gls{drt} class.}
\label{fig:gocm_drt}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_drt_pred.PNG}
\caption{Predicted for the \gls{drt} class.}
\label{fig:gocm_drt_pred}
\end{subfigure}%
}
\caption[Confusion matrices of classes trained on Inception\_v3]{On the left is the confusion matrix of the best performing Inception\_v3 and on the right is the confusion matrix on the test set.}
\label{fig:gocm}
\end{figure}

\begin{figure}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_compo_best.PNG}
\caption{Best for the Components class.}
\label{fig:gocm_comp}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-go_compo_pred.PNG}
\caption{Predicted for the Components class.}
\label{fig:gocm_comp_pred}
\end{subfigure}%
}
\caption [Performance summary of the Components class trained on Inception\_v3]{These figure are the summaries of the performance for the multi-label classification trained on Inception\_v3. It shows precision, recall, f1-score and also the number of samples for each class.}
\label{fig:perf_goog}
\end{figure}

\section{Optimizer}\label{sec:opt}
\subsection{Optimization}
Based on the results of the previous experiment, we chose to optimize further Inception\_v3. Last experiment also showed that the initialization impacted the results similarly on all labels. Our hypothesis described in \ref{sec:init} is confirmed. We can focus on Dunham for the optimization.
In this experiment we tried to capture what learning rate was best for our network. We tried the following values: 0.1, 0.01 and 0.01. In Table \ref{tab:optimlr}, we summarize the results of the different learning rate. Then in Figure \ref{fig:optim_plot}, we can see the training for each learning rate. 

\begin{table}
\caption[Results according to learning rates of Inception\_v3]{\label{tab:optimlr} The results of training Inception network using different learning rate on the Dunham class}
\centering
\begin{tabular}[b]{| l | l | l |}
\hline
    Learning rate & Validation score  & Test score\\ \hline
    0.1  &  88,9\%  & 34,4\% \\ \hline
    0.01 & 97,1\% &  68,4\%\\ \hline
    0.001 & 85,3\% &  36,3\% \\ \hline
\end{tabular} 
\end{table}

\begin{figure}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-opt_dunham_01.PNG}
\caption{Learning rate = 0.1}
\label{fig:optim_01}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-opt_dunham_001.PNG}
\caption{Learning rate = 0.01}
\label{fig:optim_001}
\end{subfigure}%
}\\
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-opt_dunham_0001.PNG}
\caption{Learning rate = 0.001}
\label{fig:optim_1}
\end{subfigure}%
}
\caption[Training with different Learning rates]{The top plot is the validation accuracy, and the bottom plot is training (in green) and validation (in yellow) losses. Those plots are the result of training Inception\_v3 on the Dunham class for different learning rates. The red line marks when the learning rate was decayed by 0.1}
\label{fig:optim_plot}
\end{figure}


\subsection{Classification}

Finally, we applied our best performing network to all the classes. Table \ref{tab:optimall} summarizes the results obtained. In Figure \ref{fig:cm_optim}, we see the best predicted confusion matrix on the test set.  

\begin{table}
\caption[Scores of best performing Inception\_v3 with \gls{sgd}]{\label{tab:optimall} The results of training Inception network using 0.01 as learning rate}
\centering
\begin{tabular}[b]{| l | l | l |}
\hline
    Class & Validation score  & Test score\\ \hline
    Dunham  &  97,1\%  & 68,4\% \\ \hline
    Porosity & 95,4\% &  65,0\%\\ \hline
    \gls{drt} & 92,6\% &  24,6\% \\ \hline
    Components & 91,1\% &  40,3\% \\ \hline
\end{tabular} 
\end{table}

\begin{figure}
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-norm_baby_65.PNG}
\caption{Predicted for Porosity}
\label{fig:optcm_poro}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-dunham_69_e28.PNG}
\caption{Predicted for Dunham}
\label{fig:optcm_dunham}
\end{subfigure}%
}\\
\makebox[\linewidth][c]{%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-norm_drt_24_e43.PNG}
\caption{Predicted for \gls{drt}}
\label{fig:optcm_drt}
\end{subfigure}%
\begin{subfigure}[b]{.6\textwidth}
\centering
\includegraphics[width=.95\textwidth]{figures/04-compo_pred_42_e24.PNG}
\caption{Predicted for Components}
\label{fig:optcm_compo}
\end{subfigure}%
}
\caption[Confusion matrices of classes predicted with Inception\_v3]{Those figures represent the Confusion Matrix of each class on the test set. The last row contains the summary of the performance for the multi-label classification. It shows precision, recall, F1-score and also the number of samples for each class}
\label{fig:cm_optim}
\end{figure}