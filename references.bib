% This file was created with JabRef 2.4.2.
% Encoding: Cp1252
%0 Data oil economist 
@article{economist,
  author  = {The Economist}, 
  title   = {The world’s most valuable resource is no longer oil, but data},
  journal = {The Economist Group Limited},
  year    = 2017,
  note    = {An optional note}, 
  biburl = {\url{https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data}}
}
@article{forbes,
  author  = {Bernard Marr}, 
  title   = {Here's Why Data Is Not The New Oil},
  journal = {Forbes},
  year    = 2018,
  note    = {An optional note}, 
  biburl = {\url{https://www.forbes.com/sites/bernardmarr/2018/03/05/heres-why-data-is-not-the-new-oil/#5eb98bf63aa9} }
}


@article{carbo,
author = {Insua, Tania L. and Hamel, Lutz and Moran, Kathryn and Anderson, Louise M. and Webster, Jody M.},
title = {Advanced classification of carbonate sediments based on physical properties},
journal = {Sedimentology},
volume = {62},
number = {2},
pages = {590-606},
keywords = {Carbonate sediment, corals, core descriptions, machine learning, microbialite, physical properties},
doi = {10.1111/sed.12168},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/sed.12168},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/sed.12168},
abstract = {Abstract Physical properties such as bulk density (gamma ray attenuation), P-wave velocity (primary or compressional wave acoustic velocity), electrical resistivity and magnetic susceptibility are related to characteristics of the marine sediments that, in turn, are indicative of the lithology. Non-destructive physical properties are routinely measured during Mission Specific Platform expeditions conducted by the Integrated Ocean Drilling Program using a multi-sensor core logger on whole cores. The goal of this study was to develop linear and non-linear relations among physical properties and different types of carbonate sediment to identify relevant information that may aid in the classification of carbonates. The database and model presented here integrate sedimentology with physical properties data. Data were analysed using three techniques: Linear Discriminant Analysis, Random Forest and Support Vector Machines. The models that best describe the nature of the data are Random Forest and Support Vector Machines, reaching up to 79\% and 74\% total accuracy, respectively. This article presents an application of machine learning as a potentially useful tool for classifying sediment types, developed specifically for assisting with the challenging identification of the lithologies in coral cores. This technique can also be used for provisional core description prior to splitting, thereby enabling identification and preservation of potentially critical intervals for special analyses and studies. These methods of data analysis can also assist with sample selection for specific studies. Other applications include the interpretation of lithotypes from wireline geophysical logging data, particularly in boreholes where core recovery is poor or sampling is limited to drill cuttings.}
}

@masterthesis{thesis_imperial,
title={Facies Classification of Carbonate Cores using Machine Learning},
author={Sharinia Kanagandran},
year={2018},
school={Imperial College London},
address={Center of Petroleum Studies}
}

%1 The big Oil and Gas book
@book{oilbegin,
  author    = {Peter Babington}, 
  title     = {Oil and Gas for Beginners},
  publisher = {Deutsche Bamk},
  year      = 2013,
 }
 @article{dunhamrevised,
title = "A revised classification of limestones",
journal = "Sedimentary Geology",
volume = "76",
number = "3",
pages = "177 - 185",
year = "1992",
issn = "0037-0738",
doi = "https://doi.org/10.1016/0037-0738(92)90082-3",
url = "http://www.sciencedirect.com/science/article/pii/0037073892900823",
author = "V.P. Wright",
abstract = "The most widely used classifications of limestones are now thirty years old and our appreciation of the diagenetic effects on limestone textures is now much greater. A revision of the classifications of Dunham (1962) and Embry and Klovan (1971) is offered and new “diagenetic” categories are proposed. The confusing term lime mudstone is replaced by calcimudstone. Two categories of diagenetic textures are recognized: non-obliterative and obliterative. The former contains limestones which are almost wholly composed of diagenetic cement (cementstones) and those whose fabric is the result of intergranular pressure solution (condensed and fitted grainstones). Obliterative categories include limestones and dolomites whose original fabric has been destroyed; sub-categories include sparstones (or dolosparstones) (crystals > 10 μm in size) and microsparstones (dolomicrosparstones) (crystals 4–10 μm in size) and dolomicrostones crystals (< 4 μm in size). Many diagenetic processes can produce fabrics which mimic depositional ones and these are reviewed."
}

%2 Dunham classification figure
 @inbook{dunhamfig,
  author       = {Al Omari, Mahmoud and Rashid, Iyad and Qinna, Nidal and Jaber, A.M. and Badwan, Adnan}, 
  title        = {Calcium Carbonates in  Profiles of Drug Substances, Excipients and Related Methodology.} ,
  chapter      = 2,
  pages        = {31-132},
  publisher    = {Academic Press},
  year         = 1993,
  volume       = 41,
}

%2+ CEloss
@Misc{celoss,
  author = "{Wikipedia contributes}", 
  title = "{Log loss}",
  howpublished = "URL: {\url{http://wiki.fast.ai/index.php/Log_Loss}}",
  note = "Accessed: 20.09.2018"
}

%3 Gradient Desecent Overview
@Misc{gradient,
  author = "{Niklas Donges}", 
  title = "{Gradient Descent in a Nutshell}",
  howpublished = "URL: {\url{https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0}}",
  note = "Accessed: 21.09.2018"
}

%4 Optimizations of gradient
@article{optimgrad,
  author    = {Sebastian Ruder},
  title     = {An overview of gradient descent optimization algorithms},
  journal   = {CoRR},
  volume    = {abs/1609.04747},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.04747},
  archivePrefix = {arXiv},
  eprint    = {1609.04747},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Ruder16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%4+ (14) Learning rate and its impact
@Misc{lr,
  author = "{Hafidz Zulkfli}", 
  title = "{Understanding Learning Rates and How It Improves Performance in Deep Learning}",
  howpublished = "URL: {\url{https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10}}",
  note = "Accessed: 25.09.2018"
}
%5 Adam and Adamax optimizers
@article{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal   = {CoRR},
  volume    = {abs/1412.6980},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.6980},
  archivePrefix = {arXiv},
  eprint    = {1412.6980},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%6 CNN performs best than humans
@ARTICLE{humanDNN,
   author = {{Geirhos}, R. and {Janssen}, D.~H.~J. and {Sch{\"u}tt}, H.~H. and 
	{Rauber}, J. and {Bethge}, M. and {Wichmann}, F.~A.},
    title = {Comparing deep neural networks against humans: object recognition when the signal gets weaker},
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1706.06969},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
     year = 2017,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170606969G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
%6+  Mutlti label classification
@Misc{multimetrics,
  author = "{William Koerhrsen}", 
  title = "{Beyond Accuracy: Precision and Recall}",
  howpublished = "URL: {\url{https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c}}",
  note = "Accessed: 15.10.2018"
}
%6bis Precision and recall
@article{metrics,
  author  = {Hossin, Mohammad and M.N, Sulaiman}, 
  title   = {A Review on Evaluation Metrics for Data Classification Evaluations},
  journal = {International Journal of Data Mining and Knowledge Management Process},
  year    = 2015,
  number  = 5,
  pages   = {01-11},
  month   = 10,
  note    = {ijdkp.2015.}, 
  volume  = 5201
}
%6ter F1 score wiki
@Misc{wiki-f1,
  author = "{Wikipedia contributes}", 
  title = "{F1-score}",
  howpublished = "URL: {\url{https://en.wikipedia.org/wiki/F1_score}}",
  note = "Accessed: 10.10.2018"
}
%6car Confusion matrix image
@Misc{cm_image,
  author = "{Sebastian Raschka}", 
  title = "{Confusion matrix}",
  howpublished = "URL: {\url{https://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix/confusion-matrix}}",
  note = "Accessed: 10.10.2018"
}
%6 fem CM 
@Misc{cm,
  author = "{Sarang Narkhede}", 
  title = "{Understanding Confusion Matrix}",
  howpublished = "URL: {\url{https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62}}",
  note = "Accessed: 10.10.2018"
}

%7 Elastic search 
@proceedings{elastic,
editor = {Simard, Patrice Y. and Steinkraus, Dave and Platt, John},
title = {Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis},
year = {2003},
month = {8},
abstract = {
Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple “do-it-yourself” implementation of convolution with a flexible architecture is suitable for many visual document problems. This simple convolutional neural network does not require complex methods, such as momentum, weight decay, structure dependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.
},
publisher = {Institute of Electrical and Electronics Engineers, Inc.},
url = {https://www.microsoft.com/en-us/research/publication/best-practices-for-convolutional-neural-networks-applied-to-visual-document-analysis/}
}

%8 Resnet paper
@article{resnetpaper,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeZRS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%8bis Medium resnet
@Misc{mediumresnet,
  author = "{Prakash Jay}", 
  title = "{Understanding and Implementing Architectures of ResNet and ResNeXt for state-of-the-art Image Classification: From Microsoft to Facebook}",
  howpublished = "URL: {\url{https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624}}",
  note = "Accessed: 15.09.2018"
}
%9 Alexnet paper
@article{alexpaper,
  author    = {Alex Krizhevsky},
  title     = {One weird trick for parallelizing convolutional neural networks},
  journal   = {CoRR},
  volume    = {abs/1404.5997},
  year      = {2014},
  url       = {http://arxiv.org/abs/1404.5997},
  archivePrefix = {arXiv},
  eprint    = {1404.5997},
  timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Krizhevsky14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%10 Googlenet paper
@article{googlepaper,
  author    = {Christian Szegedy and
               Wei Liu and
               Yangqing Jia and
               Pierre Sermanet and
               Scott E. Reed and
               Dragomir Anguelov and
               Dumitru Erhan and
               Vincent Vanhoucke and
               Andrew Rabinovich},
  title     = {Going Deeper with Convolutions},
  journal   = {CoRR},
  volume    = {abs/1409.4842},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.4842},
  archivePrefix = {arXiv},
  eprint    = {1409.4842},
  timestamp = {Mon, 13 Aug 2018 16:48:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SzegedyLJSRAEVR14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%11 Transfer Learning Survey
@article{translearnsurvey,
  author  = {Sinno Jialin Pan and Qiang Yang}, 
  title   = {A Survey on Transfer Learning},
  journal = { IEEE Transactions on Knowledge and Data Engineering },
  year    = 2009,
  number  = 10,
  pages   = {1345-1359},
  month   = 10,
  note    = { }, 
  volume  = 22
}
%11bis Early layers learn differently
@article{layerslearn,
  author    = {Jason Yosinski and
               Jeff Clune and
               Yoshua Bengio and
               Hod Lipson},
  title     = {How transferable are features in deep neural networks?},
  journal   = {CoRR},
  volume    = {abs/1411.1792},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.1792},
  archivePrefix = {arXiv},
  eprint    = {1411.1792},
  timestamp = {Mon, 13 Aug 2018 16:47:20 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/YosinskiCBL14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%11tris Thin section
@Misc{section,
  author = "{SEPM STRATA}", 
  title = "{Hardened Peloid in Capillary Crust - Holocene}",
  howpublished = "URL: {\url{http://www.sepmstrata.org/microscopic_gallery_details.aspx?gid=224&pg=2&gcid=10}}",
  note = "Accessed: 15.10.2018"
}
%11 Deep Learning book
@book{deepbook,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

%12 Neural nets and DL
@misc{nnbook,
  added-at = {2018-02-02T15:47:03.000+0100},
  author = {Nielsen, Michael A.},
  biburl = {https://www.bibsonomy.org/bibtex/274383acee84241145ff4ffede9658206/martin29},
  interhash = {04d527cadd39f888fc3babcad3343362},
  intrahash = {74383acee84241145ff4ffede9658206},
  keywords = {Neural and book deep learning networks},
  publisher = {Determination Press},
  timestamp = {2018-02-02T15:47:03.000+0100},
  title = {Neural Networks and Deep Learning},
  type = {misc},
  url = {http://neuralnetworksanddeeplearning.com/},
  year = 2018
}

%13 Stanford course
@misc{cs231n,
    year = {2015},
    title = {CS231n: Convolutional Neural Networks for Visual Recognition},
    author = {Stanford University},
    url = {http://cs231n.stanford.edu/syllabus.html}
}


